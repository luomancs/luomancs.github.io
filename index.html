<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/favicon.ico">
    <title>Man Luo</title>
    <style type="text/css">
    .center {
        text-align: center;
    }

    .lead {
        font-size: 0.9rem;
        text-align: left;
    }
        
    .subtitle {
        font-size: 1.5rem;
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
    }
        
    .title {
        font-size: 1.8rem;
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
        font-weight: 450;
    }
    
    .ttitle {
        font-size: 1.2rem;
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
    }

    .links {
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
    }

    .yyyymm {
        float: left;
        width: 80px;
        /* font-weight: bold; */
        text-align: center;
    }
    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149582119-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'UA-149582119-2');
    </script>

</head>

<body>
    <div class="container" style="margin-top:10px;">
        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <!-- Intro -->
                <div>
                    <div class="row">
                        <div class="col-sm-3" align="middle">
                            <img src="images/manluo4.PNG" alt="" class="figure-img img-fluid" alt="Responsive image" style="margin:0px;margin-top:10px;margin-bottom:10px">
                        </div>
                        <div class="col" style="margin-top: 15px; margin-bottom: auto; margin-left:5px">
                            <header>
                                <h3 class="title">Man Luo (罗满) </h3>
                                <h3 class="ttitle">Ph.D.</h3>
				                <div class="links">mluo26 [at] asu [dot] edu</h3>
				                <div class="links">
                                    <a href='https://scholar.google.com/citations?hl=en&user=RFeq08UAAAAJ'>Google Scholar</a> / 
				                    <a href='https://github.com/luomancs'>Github</a> / 
                                    <a href='https://www.linkedin.com/in/man-luo-a7aa57178/'>LinkedIn</a> / 
                                    <a href='man_luo_resume.pdf'>CV</a>
                                </div>
                            </header>
                        </div>
                    </div>
                </div>
                <!-- About Me -->
                <hr>
                <div>
                    <h3 class="subtitle">About</h3>
                    <p class="lead">
                        I am currently an AI Research Scientist at <a href="https://www.intel.com/content/www/us/en/research/overview.html">Intel Lab</a>, working on large scale multimodal system pretraining and multimodal retrieval augmented generation (RAG). 
                    </p>
                    <p class="lead">
                        My research dives deep into the realms of information retrieval and reading comprehension within natural language processing (NLP) and multimodal domains. The primary goal of my research is to develop advanced models that not only efficiently retrieve and utilize external knowledge for enriched comprehension and reasoning but also demonstrate superior generalization capabilities across unfamiliar tasks and domains. A few areas of my recent interest include:
                        <ul class="lead" style="margin-bottom:7px">
                            <li><b>Knowledge Retrieval:</b> How can we effectively retrieve and utilize external knowledge to not only enhance comprehension but also mitigate hallucination?</li>
                            <li><b>Generalization:</b> Designing models that can seamlessly adapt and perform across various tasks and domains without explicit training.</li>
                            <li><b>Multimodal Understanding:</b> Delving into the integration of textual, visual, and other modalities to bolster machine comprehension and response capabilities.</li>
                            <li><b>Biomedical/healthcare application and innovation:</b> Evaluate and innovate the LLMs to solve biomedical and healthcare challenges, such as long sequence processing, noisy data mitigation, data imbalance rectification, and enhancing interpretability.</li> 
                        </ul>
                    </p>
                    <p class="lead">
                        Previously, I was a research fellow at Mayo Clinic, AZ, working closely with <a href="https://labs.engineering.asu.edu/banerjeelab/person/imon-banerjee/">Dr. Imon Banerjee</a> and <a href="https://scai.engineering.asu.edu/tenured-and-tenure-track/associate-professor/bhavik-n-patel/">Dr. Bhavik Patel</a>. I earned my doctoral degree in 2023 from Arizona State University under the esteemed supervision of <a href="https://www.public.asu.edu/~cbaral/">Dr. Chitta Baral</a>. I am also privileged to collaborate with amazing industry researchers from Salesforce, Meta and Google during my internship.
                    </p>
                </div>
                <!-- News -->
                <hr>
                <div>
                    <h3 class="subtitle">News</h3>
                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2024</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2024.10]:</span> Our paper <a href="https://arxiv.org/abs/2405.16402">EMRank</a> has been accepted to IEEE Big Data for Healthcare!</li>
                            <li><span class="yyyymm">[2024.10]:</span> Our paper <a href="https://arxiv.org/pdf/2403.16422">SA-OcrPaint</a> has been accepted to WACV 2025!</li>
                            <li><span class="yyyymm">[2024.8]:</span> Our Survey paper <a href="https://arxiv.org/pdf/2401.11624.pdf">Ret-ICL</a> has been accepted to TMLR!</li>
                            <li><span class="yyyymm">[2024.07]:</span> New Paper <a href="https://arxiv.org/pdf/2406.19593">SK-VQA</a> is out! A synthetic generated VQA dataset improves multimodel content-based generalization capacity!</li>
                            <li><span class="yyyymm">[2024.06]:</span> New Book <a href="https://link.springer.com/book/10.1007/978-3-031-57816-8">Advances in Multimodal Information Retrieval and Generation </a> is out! </li>
                            <li><span class="yyyymm">[2024.03]:</span> New Paper <a href="https://arxiv.org/pdf/2403.16422.pdf">SA-OcrPaint </a> is out! A training-free framework to improve the Diffusion Model ability to generate visual text!</li>
                            <li><span class="yyyymm">[2024.03]:</span> New Paper <a href="https://www.medrxiv.org/content/10.1101/2024.03.15.24304362v3.full-text">PCA-LLM </a> is a domain specific LLM for Prostate Cancer, we train a GPT-2 type of model from scratch on Prostate Cancer clinical notes with domain specific vocabulary.</li>
                            <li><span class="yyyymm">[2024.03]:</span> I am very excited to join Intel AI Lab, CA, as a AI research Scientist.</li>
                            <li><span class="yyyymm">[2024.02]:</span> <a href="https://luomancs.github.io/Multimodal4Health-ICHI/">Call for Paper: Multimodal4health Workshop. </a> Will be held at <a href="https://ieeeichi2024.github.io/">ICHI Conference</a> in Orlando, Florida in June 3rd 2024.</li>
                            <li><span class="yyyymm">[2024.01]:</span> <a href="https://arxiv.org/pdf/2401.11624.pdf">Ret-ICL</a> survey paper is out: In-context Learning with Retrieved Demonstrations for Language Models: A Survey.</li>
                        </ul>
                    </div>
                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2023</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2023.11]:</span> I will attend <a href="https://www.rsna.org/">RSNA 2023</a> and give an oral presentaion and an invited course section on Nov.27th. Stop by and say Hi :)</li>
                            <li><span class="yyyymm">[2023.10]:</span> <a href="https://arxiv.org/pdf/2310.00836.pdf">LogiGLUE</a> paper is out</a>: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models.</li>
                            <li><span class="yyyymm">[2023.07]:</span> A new multimodal-query retriever dataset is public available on github! Check the <a href="https://github.com/luomancs/ReMuQ">dataset</a> and the <a href="https://arxiv.org/abs/2306.00424">paper</a>.</li>
                            <li><span class="yyyymm">[2023.07]:</span> Join Mayo Clinic as a Research Fellow! </li>
                            <li><span class="yyyymm">[2023.05]:</span> Our work on the <a href="https://arxiv.org/pdf/2305.12096.pdf">commonsense assumption of language models </a>is available on arXiv, also check the <a href="https://github.com/nrjvarshney/break_the_common_assumptions">dataset</a>!</li>
                            <li><span class="yyyymm">[2023.05]:</span> My Google internship work on <a href="https://arxiv.org/pdf/2305.14128.pdf">Dr.ICL: Demonstration-Retrieved In-context Learning</a> is available on arXiv, check it out!</li>
                            <li><span class="yyyymm">[2023.05]:</span> Our work on end-to-end multimodal retriever has been accepted to <b>ACL 23!</b></li>
                            <li><span class="yyyymm">[2023.05]:</span> My Meta internship work on hybrid efficient retriever has been accepted to <b>ACL 23</b>, check out <a href="https://arxiv.org/pdf/2210.01371.pdf"> our paper</a>! </li>
                            <li><span class="yyyymm">[2023.04]:</span> Successfully defense my Ph.D. dissertation: <a href="https://www.proquest.com/docview/2813838506?pq-origsite=gscholar&fromopenview=true">Neural Retrieval and Reader for Information Retrieval and Question Answering</a> <a href="https://drive.google.com/file/d/1cZ2qUDtAD8bDB3_Oqp7aVyTAKx4LWXMp/view?usp=sharing">[Slide]</a></li>
                            <li><span class="yyyymm">[2023.01]:</span> Invited talk at <a href = 'https://asu-apg.github.io/serum/'>SERUM @ WACV 2023</a>, <a href='https://drive.google.com/file/d/1B5Q22ew62Sq0-rX6f3In4UL8kbNkK9jq/view?usp=sharing'>[Slide]</a></li>
                        </ul>
                    </div>
                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2022</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2022.12]:</span><b>2th O-DRUM Workshop</b> will be back at CVPR 2023 with a new rhythm: Open-Domain *Reasoning* under Multimodal settings.</li>
                            <li><span class="yyyymm">[2022.08]:</span> Start internship at <b>Google Research</b>.</li>
                            <li><span class="yyyymm">[2022.08]:</span> Completed <a href="https://arxiv.org/pdf/2205.16005.pdf">thesis proposal</a> and became Ph.D. candidate.</li>
                            <li><span class="yyyymm">[2022.05]:</span> Start internship at <b>Meta Reality Lab </b> working on efficient retrieval task.</li>
                            <li><span class="yyyymm">[2022.05]:</span> 2 papers accpeted to <b>NAACL 2022 SRW </b>.</li>
                            <li><span class="yyyymm">[2022.04]:</span> 1 paper accpeted to <b>NAACL 2022 Finging</b>.</li>
                            <li><span class="yyyymm">[2022.04]:</span> 1 paper accpeted to <b>ACL 2022 Spa-NLP workshop</b>.</li>
                            <li><span class="yyyymm">[2022.02]:</span> 1 paper accpeted to <b>ACL 2022 Finding</b>.</li>
                            <li><span class="yyyymm">[2022.01]:</span> <a href = 'https://asu-apg.github.io/odrum/'>O-Drum</a> Workshop will be held in <b>CVPR 2022</b></li>
                        </ul>
                    </div>
                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2021</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2021.12]:</span> 1 paper accepted to <b>AAAI 2021</b>. Acceptant rate 15%.</li>
                            <li><span class="yyyymm">[2021.08]:</span> 1 paper accepted to <b>EMNLP 2021</b>.</li>
                            <li><span class="yyyymm">[2021.06]:</span> Yankai Zeng (mentor by me) passed his master thesis and now be a Ph.D. student in <a href='https://www.utdallas.edu/'>UTD</a>.</li>
                            <li><span class="yyyymm">[2021.05]:</span> Research intern at Salesforce and worked with <a href=''>Kazuma Hashimoto</a> and <a href='https://scholar.google.com/citations?user=H_6RQ7oAAAAJ&hl=en'>Yingbo Zhou</a>.</li>
                            <li><span class="yyyymm">[2021.04]:</span> Finalist of 2021 <a href='https://newcollege.asu.edu/asu-graduate-college-recognizes-research-2021-knowledge-mobilization-awards'>Knowledge Mobilization Awards</a>.</li>
                            <li><span class="yyyymm">[2021.03]:</span>Invited talk at exploreCSR workshop (ASU).</li>
                            <li><span class="yyyymm">[2021.02]:</span> 1 paper accepted to <b>EACL 2021</b>.</li>
                        </ul>
                    </div>

                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2019</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2019.09]:</span> 1 paper accpeted to <b>ICLP 2019</b>.</li>
                            <li><span class="yyyymm">[2019.09]:</span> Presented at <b>ICLP 2019</b> <a href="https://sites.google.com/cs.stonybrook.edu/iclp2019dc/iclp-2019-doctoral-consortium">Doctoral Consortium</a></li>
                            <li><span class="yyyymm">[2019.06]:</span> 1 workshop paper accepted to <b>ASPOCP 2019 </b></li>
                        </ul>
                    </div>

                </div>
                <hr>
                <div>
                    <h3 class="subtitle">Professional Activities and Experience</h3>
                    <div>
                        <ul class="lead" style="margin-bottom:7px">
                            <li> Guest Editor of PLOS Digital Medicine </li>
                            <li> Workshop orginzer for O-DRUM <a href="https://asu-apg.github.io/odrum/">2023</a>, <a href="https://asu-apg.github.io/odrum/archive_2022.html">2022</a> </li>
                            <li> Reviewer for AAAI, NIPS, EMNLP, ACL, EACL </li>
                            </ul>
                    </div>
                </div>

                
                <hr>
                <div>
                    <h3 class="subtitle"> Publications</h3>
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">End-to-end Knowledge Retrieval with Multi-modal Queries </span></br>
                                <span class="font-weight-bold">Man Luo</span>, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, Chitta Baral</br>
                                ACL 2023</br>
				                <a href='https://arxiv.org/pdf/2306.00424.pdf'>[Paper]</a>
                            </p>
                        </div>
                    </div>

                    <div class="row">

                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">A Study on the Efficiency and Generalization of Light Hybrid Retrievers</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Shashank Jain, Anchit Gupta, Arash Einolghozati, Barlas Oguz Debojeet Chatterjee, Xilun Chen, Chitta Baral, Peyman Heidari</br>
                                ACL 2023</br>
				                <a href='https://arxiv.org/pdf/2210.01371.pdf'>[Paper]</a>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">In-BoXBART: Get Instructions into Biomedical Multi-Task Learning</span></br>
                                Mihir Parmar, Swaroop Mishra, Mirali Purohit, <span class="font-weight-bold">Man Luo</span>, M. Hassan Murad, Chitta Baral</br>
                                NAACL 2022 Finding</br>
				                <a href='https://arxiv.org/pdf/2204.07600.pdf'>[Paper]</a>
                                <a href="https://huggingface.co/cogint/in-boxbart">[Model in Huggingface]</a>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness</span></br>
                                Tejas Gokhale*, Swaroop Mishra*, <span class="font-weight-bold">Man Luo*</span>, Bhavdeep Singh Sachdeva, Chitta Baral</br>
                                ACL 2022 Finding</br>
				                <a href='https://aclanthology.org/2022.findings-acl.213.pdf'>[Paper]</a>
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Improving Biomedical Information Retrieval with Neural Retrievers</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Arindam Mitra, Tejas Gokhale, Chitta Baral</br>
                                AAAI 2022 </br>
				                <a href='https://arxiv.org/pdf/2201.07745.pdf'>[Paper]</a>
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering</span></br>
                                <span class="font-weight-bold">Man Luo*</span>, Yankai Zeng*, Pratyay Banerjee, Chitta Baral</br>
                                EMNLP 2021 </br>
				                <a href='https://arxiv.org/pdf/2109.04014.pdf'>[Paper]</a> <a href='https://github.com/luomancs/retriever_reader_for_okvqa'>[Code]</a>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">‘Just because you are right, doesn’t mean I am wrong’: Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Shailaja Keyur Sampat, Riley Tallman, Yankai Zeng, Manuha Vancha, Akarshan Sajja, Chitta Baral</br>
                                EACL 2021 </br>
				                <a href='https://aclanthology.org/2021.eacl-main.240.pdf'>[Paper]</a> 
                                <a href="https://github.com/luomancs/alternative_answer_set">[Code]</a>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Strong equivalence for LPMLN programs</span></br>
                                Joohyung Lee and <span class="font-weight-bold">Man Luo</span></br>
                                ICLP 2019 </br>
				                <a href='https://arxiv.org/pdf/1909.08998.pdf'>[Paper]</a> 
                            </p>
                        </div>
                    </div>
                </div>

                
                <!-- Preprint Papers -->
                <hr>
                <div>
                    <h3 class="subtitle">Preprinted Papers</h3>
                    <!-- OBQA -->
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Shuguang Chen, Chitta Baral.</br>
				                arXiv preprint </br>
				                <a href='https://arxiv.org/abs/2109.10497'>[Paper]</a> <a href='https://github.com/luomancs/joint_model'>[Code]</a>
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Can Transformers Reason About Effects of Actions?</span></br>
                                Pratyay Banerjee, Chitta Baral, <span class="font-weight-bold"></span>Man Luo</span>, Arindam Mitra, Kuntal Pal, Tran C Son, Neeraj Varshney
				                arXiv preprint </br>
				                <a href='https://arxiv.org/pdf/2012.09938.pdf'>[Paper]</a>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="col-md-2"></div>
            </div>
    <!-- </div> -->
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
</body>

</html>
