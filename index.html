<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/favicon.ico">
    <title>Man Luo</title>
    <style type="text/css">
    .center {
        text-align: center;
    }

    .lead {
        font-size: 0.9rem;
        text-align: left;
    }
        
    .subtitle {
        font-size: 1.5rem;
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
    }
        
    .title {
        font-size: 1.8rem;
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
        font-weight: 450;
    }
    
    .ttitle {
        font-size: 1.2rem;
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
    }

    .links {
        text-align: left;
        font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;
    }

    .yyyymm {
        float: left;
        width: 80px;
        /* font-weight: bold; */
        text-align: center;
    }
    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149582119-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'UA-149582119-2');
    </script>

</head>

<body>
    <div class="container" style="margin-top:10px;">
        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <!-- Intro -->
                <div>
                    <div class="row">
                        <div class="col-sm-3" align="middle">
                            <img src="images/manluo4.PNG" alt="" class="figure-img img-fluid" alt="Responsive image" style="margin:0px;margin-top:10px;margin-bottom:10px">
                        </div>
                        <div class="col" style="margin-top: 15px; margin-bottom: auto; margin-left:5px">
                            <header>
                                <h3 class="title">Man Luo (罗满) </h3>
                                <h3 class="ttitle">Ph.D.</h3>
				                <div class="links">mluo26 [at] asu [dot] edu</h3>
				                <div class="links">
                                    <a href='https://scholar.google.com/citations?hl=en&user=RFeq08UAAAAJ'>Google Scholar</a> / 
				                    <a href='https://github.com/luomancs'>Github</a> / 
                                    <a href='https://www.linkedin.com/in/man-luo-a7aa57178/'>LinkedIn</a> / 
                                    <a href='Man_Luo_Resume-0806.pdf'>CV</a>
                                </div>
                            </header>
                        </div>
                    </div>
                </div>
                <!-- About Me -->
                <hr>
                <div>
                    <h3 class="subtitle">About</h3>
                    <p class="lead">
                        I am currently an AI Research Scientist at <a href="https://www.intel.com/content/www/us/en/research/overview.html">Intel Lab</a>, multimodal cognitive AI team. My research includes                         Multimodal and language models post-training, Multimodal Retrieval and Generation, Synthetic Data Pipelines, Computer Use Agents, and Multimodal Applications in Healthcare. More details:
                    </p>
                    <p class="lead">
                        <!-- My research dives deep into the realms of information retrieval and reading comprehension within natural language processing (NLP) and multimodal domains. The primary goal of my research is to develop advanced models that not only efficiently retrieve and utilize external knowledge for enriched comprehension and reasoning but also demonstrate superior generalization capabilities across unfamiliar tasks and domains. A few areas of my recent interest include: -->
                        <ul class="lead" style="margin-bottom:7px">
                            <li><b>Knowledge Retrieval:</b> How can we effectively retrieve and utilize external knowledge to not only enhance comprehension but also mitigate hallucination?</li>
                            <li><b>Generalization:</b> Designing models that can seamlessly adapt and perform across various tasks and domains without explicit training.</li>
                            <li><b>Multimodal Understanding:</b> Delving into the integration of textual, visual, and other modalities to bolster machine comprehension and response capabilities.</li>
                            <li><b>Biomedical/healthcare application and innovation:</b> Evaluate and innovate the LLMs to solve biomedical and healthcare challenges, such as long sequence processing, noisy data mitigation, data imbalance rectification, and enhancing interpretability.</li> 
                        </ul>
                    </p>
                    <p class="lead">
                        Previously, I was a research fellow at Mayo Clinic, AZ, working closely with <a href="https://labs.engineering.asu.edu/banerjeelab/person/imon-banerjee/">Dr. Imon Banerjee</a> and <a href="https://scai.engineering.asu.edu/tenured-and-tenure-track/associate-professor/bhavik-n-patel/">Dr. Bhavik Patel</a>. I earned my doctoral degree in 2023 from Arizona State University under the esteemed supervision of <a href="https://www.public.asu.edu/~cbaral/">Dr. Chitta Baral</a>. I am also privileged to collaborate with amazing industry researchers from Salesforce, Meta and Google during my internship.
                    </p>
                </div>
                <!-- News -->
                <hr>
                <div>
                    <h3 class="subtitle">News</h3>
                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2025</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2025.06]:</span> New paper <a href="https://arxiv.org/abs/2506.03095">DPO </a>Learning with LLMs-Judge Signal for Computer Use Agents.</li>
                            <li><span class="yyyymm">[2025.05]:</span> Our paper <a href="https://arxiv.org/pdf/2406.19593">SK-VQA</a> has been accepted to ICML 2025 Oral Presentation (less than 1%).</li>
                            <li><span class="yyyymm">[2025.05]:</span> Our paper <a href="https://arxiv.org/pdf/2412.03092">Revolve</a> has been accepted to ICML 2025.</li>
                            <li><span class="yyyymm">[2025.03]:</span> New paper <a href="https://arxiv.org/pdf/2412.03092">FiVL</a> A Framework for Improved Vision-Language Alignment is out.</li>    
                        </ul>
                    </div>
                    <div>
                        <h5 style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif;"> <b>2024</b> </h5>
                        <ul class="lead" style="margin-bottom:7px">
                            <li><span class="yyyymm">[2024.10]:</span> Our paper <a href="https://arxiv.org/abs/2405.16402">EMRank</a> has been accepted to IEEE Big Data for Healthcare!</li>
                            <li><span class="yyyymm">[2024.10]:</span> Our paper <a href="https://arxiv.org/pdf/2403.16422">SA-OcrPaint</a> has been accepted to WACV 2025!</li>
                            <li><span class="yyyymm">[2024.08]:</span> Our Survey paper <a href="https://arxiv.org/pdf/2401.11624.pdf">Ret-ICL</a> has been accepted to TMLR!</li>
                            <li><span class="yyyymm">[2024.07]:</span> New Paper <a href="https://arxiv.org/pdf/2406.19593">SK-VQA</a> is out!</li>
                            <li><span class="yyyymm">[2024.06]:</span> New Book <a href="https://link.springer.com/book/10.1007/978-3-031-57816-8">Advances in Multimodal Information Retrieval and Generation </a> is out! </li>
                            <li><span class="yyyymm">[2024.03]:</span> I am very excited to join Intel AI Lab, CA, as a AI research Scientist.</li>
                            <li><span class="yyyymm">[2024.02]:</span> <a href="https://luomancs.github.io/Multimodal4Health-ICHI/">Call for Paper: Multimodal4health Workshop. </a> Will be held at <a href="https://ieeeichi2024.github.io/">ICHI Conference</a>.</li>
                            <li><span class="yyyymm">[2024.01]:</span> Survey: <a href="https://arxiv.org/pdf/2401.11624.pdf">Ret-ICL</a> In-context Learning with Retrieved Demonstrations for Language Models.</li>
                        </ul>
                    </div>
                    <!-- Collapsible News Sections Start Here -->
                    <div id="newsAccordion">
                        <div class="d-flex flex-row mb-2" style="gap: 20px;">
                            <a class="collapsed" data-toggle="collapse" href="#news2023" role="button" aria-expanded="false" aria-controls="news2023" style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif; text-decoration: none; font-size: 1.2rem;">
                                <b>2023</b>
                            </a>
                            <a class="collapsed" data-toggle="collapse" href="#news2022" role="button" aria-expanded="false" aria-controls="news2022" style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif; text-decoration: none; font-size: 1.2rem;">
                                <b>2022</b>
                            </a>
                            <a class="collapsed" data-toggle="collapse" href="#news2021" role="button" aria-expanded="false" aria-controls="news2021" style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif; text-decoration: none; font-size: 1.2rem;">
                                <b>2021</b>
                            </a>
                            <a class="collapsed" data-toggle="collapse" href="#news2019" role="button" aria-expanded="false" aria-controls="news2019" style="font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif; text-decoration: none; font-size: 1.2rem;">
                                <b>2019</b>
                            </a>
                        </div>
                        <div class="collapse" id="news2023" data-parent="#newsAccordion">
                            <ul class="lead" style="margin-bottom:7px">
                                <li><span class="yyyymm">[2023.11]:</span> I will attend <a href="https://www.rsna.org/">RSNA 2023</a> and give an oral presentaion and an invited course section.</li>
                                <li><span class="yyyymm">[2023.10]:</span> New Paper: <a href="https://arxiv.org/pdf/2310.00836.pdf">LogiGLUE</a>: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models.</li>
                                <li><span class="yyyymm">[2023.07]:</span> A new multimodal-query retriever dataset is public available on github! Check the <a href="https://github.com/luomancs/ReMuQ">dataset</a> and the <a href="https://arxiv.org/abs/2306.00424">paper</a>.</li>
                                <li><span class="yyyymm">[2023.07]:</span> Join Mayo Clinic as a Research Fellow! </li>
                                <li><span class="yyyymm">[2023.05]:</span> Our work on the <a href="https://arxiv.org/pdf/2305.12096.pdf">commonsense assumption of language models </a>is available on arXiv, also check the <a href="https://github.com/nrjvarshney/break_the_common_assumptions">dataset</a>!</li>
                                <li><span class="yyyymm">[2023.05]:</span> My Google internship work on <a href="https://arxiv.org/pdf/2305.14128.pdf">Dr.ICL: Demonstration-Retrieved In-context Learning</a> is available on arXiv, check it out!</li>
                                <li><span class="yyyymm">[2023.05]:</span> Our work on end-to-end multimodal retriever has been accepted to <b>ACL 23!</b></li>
                                <li><span class="yyyymm">[2023.05]:</span> My Meta internship work on hybrid efficient retriever has been accepted to <b>ACL 23</b>, check out <a href="https://arxiv.org/pdf/2210.01371.pdf"> our paper</a>! </li>
                                <li><span class="yyyymm">[2023.04]:</span> Successfully defense my Ph.D. dissertation: <a href="https://www.proquest.com/docview/2813838506?pq-origsite=gscholar&fromopenview=true">Neural Retrieval and Reader for Information Retrieval and Question Answering</a> <a href="https://drive.google.com/file/d/1cZ2qUDtAD8bDB3_Oqp7aVyTAKx4LWXMp/view?usp=sharing">[Slide]</a></li>
                                <li><span class="yyyymm">[2023.01]:</span> Invited talk at <a href = 'https://asu-apg.github.io/serum/'>SERUM @ WACV 2023</a>, <a href='https://drive.google.com/file/d/1B5Q22ew62Sq0-rX6f3In4UL8kbNkK9jq/view?usp=sharing'>[Slide]</a></li>
                            </ul>
                        </div>
                        <div class="collapse" id="news2022" data-parent="#newsAccordion">
                            <ul class="lead" style="margin-bottom:7px">
                                <li><span class="yyyymm">[2022.12]:</span><b> 2th O-DRUM Workshop</b> will be back at CVPR 2023 with a new rhythm: Open-Domain *Reasoning* under Multimodal settings.</li>
                                <li><span class="yyyymm">[2022.08]:</span> Start internship at <b>Google Research</b>.</li>
                                <li><span class="yyyymm">[2022.08]:</span> Completed <a href="https://arxiv.org/pdf/2205.16005.pdf">thesis proposal</a> and became Ph.D. candidate.</li>
                                <li><span class="yyyymm">[2022.05]:</span> Start internship at <b>Meta Reality Lab </b> working on efficient retrieval task.</li>
                                <li><span class="yyyymm">[2022.05]:</span> 2 papers accpeted to <b>NAACL 2022 SRW </b>.</li>
                                <li><span class="yyyymm">[2022.04]:</span> 1 paper accpeted to <b>NAACL 2022 Finging</b>.</li>
                                <li><span class="yyyymm">[2022.04]:</span> 1 paper accpeted to <b>ACL 2022 Spa-NLP workshop</b>.</li>
                                <li><span class="yyyymm">[2022.02]:</span> 1 paper accpeted to <b>ACL 2022 Finding</b>.</li>
                                <li><span class="yyyymm">[2022.01]:</span> <a href = 'https://asu-apg.github.io/odrum/'>O-Drum</a> Workshop will be held in <b>CVPR 2022</b></li>
                            </ul>
                        </div>
                        <div class="collapse" id="news2021" data-parent="#newsAccordion">
                            <ul class="lead" style="margin-bottom:7px">
                                <li><span class="yyyymm">[2021.12]:</span> 1 paper accepted to <b>AAAI 2021</b>. Acceptant rate 15%.</li>
                                <li><span class="yyyymm">[2021.08]:</span> 1 paper accepted to <b>EMNLP 2021</b>.</li>
                                <li><span class="yyyymm">[2021.06]:</span> Yankai Zeng (mentor by me) passed his master thesis and now be a Ph.D. student in <a href='https://www.utdallas.edu/'>UTD</a>.</li>
                                <li><span class="yyyymm">[2021.05]:</span> Research intern at Salesforce and worked with <a href=''>Kazuma Hashimoto</a> and <a href='https://scholar.google.com/citations?user=H_6RQ7oAAAAJ&hl=en'>Yingbo Zhou</a>.</li>
                                <li><span class="yyyymm">[2021.04]:</span> Finalist of 2021 <a href='https://newcollege.asu.edu/asu-graduate-college-recognizes-research-2021-knowledge-mobilization-awards'>Knowledge Mobilization Awards</a>.</li>
                                <li><span class="yyyymm">[2021.03]:</span>Invited talk at exploreCSR workshop (ASU).</li>
                                <li><span class="yyyymm">[2021.02]:</span> 1 paper accepted to <b>EACL 2021</b>.</li>
                            </ul>
                        </div>
                        <div class="collapse" id="news2019" data-parent="#newsAccordion">
                            <ul class="lead" style="margin-bottom:7px">
                                <li><span class="yyyymm">[2019.09]:</span> 1 paper accpeted to <b>ICLP 2019</b>.</li>
                                <li><span class="yyyymm">[2019.09]:</span> Presented at <b>ICLP 2019</b> <a href="https://sites.google.com/cs.stonybrook.edu/iclp2019dc/iclp-2019-doctoral-consortium">Doctoral Consortium</a></li>
                                <li><span class="yyyymm">[2019.06]:</span> 1 workshop paper accepted to <b>ASPOCP 2019 </b></li>
                            </ul>
                        </div>
                    </div>
                    <!-- Collapsible News Sections End Here -->
                </div>
                <hr>
                <div>
                    <h3 class="subtitle">Professional Activities and Experience</h3>
                    <div>
                        <ul class="lead" style="margin-bottom:7px">
                            <li> Super Volunteer for WiML @NeurIPS 2024.</li>
                            <li> Editor of PLOS Digital Medicine </li>
                            <li> Guest Editor of PLOS Digital Medicine </li>
                            <li> Workshop orginzer for Multimodal4Health <a href="https://luomancs.github.io/Multimodal4Health-ICHI/">2024</a> </li>
                            <li> Workshop orginzer for O-DRUM <a href="https://asu-apg.github.io/odrum/">2023</a>, <a href="https://asu-apg.github.io/odrum/archive_2022.html">2022</a> </li>
                            <li> Reviewer for AAAI, NIPS, EMNLP, ACL, EACL </li>
                            </ul>
                    </div>
                </div>

                
                <hr>
                <div>
                    <h3 class="subtitle"> Publications</h3>
                    
                    <div class="links">
                        <a href='https://scholar.google.com/citations?hl=en&user=RFeq08UAAAAJ'>Full List of Publications</a> 
                    </div>  
                    <br>
                    <div class="row">
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">In-BoXBART: Get Instructions into Biomedical Multi-Task Learning</span></br>
                                Mihir Parmar, Swaroop Mishra, Mirali Purohit, <span class="font-weight-bold">Man Luo</span>, M. Hassan Murad, Chitta Baral</br>
                                NAACL 2022 Finding</br>
				                <a href='https://arxiv.org/pdf/2204.07600.pdf'>[Paper]</a>
                                <a href="https://huggingface.co/cogint/in-boxbart">[Model in Huggingface]</a>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">In-context Learning with Retrieved Demonstrations for Language Models: A Survey</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi</br>
                                TACL 2024</br>
				                <a href='https://arxiv.org/pdf/2401.11624'>[Paper]</a>
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Improving Biomedical Information Retrieval with Neural Retrievers</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Arindam Mitra, Tejas Gokhale, Chitta Baral</br>
                                AAAI 2022 </br>
				                <a href='https://arxiv.org/pdf/2201.07745.pdf'>[Paper]</a>
                            </p>
                        </div>
                    </div>

                    
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">‘Just because you are right, doesn’t mean I am wrong’: Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Shailaja Keyur Sampat, Riley Tallman, Yankai Zeng, Manuha Vancha, Akarshan Sajja, Chitta Baral</br>
                                EACL 2021 </br>
				                <a href='https://aclanthology.org/2021.eacl-main.240.pdf'>[Paper]</a> 
                                <a href="https://github.com/luomancs/alternative_answer_set">[Code]</a>
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Dr. ICL: Demonstration-Retrieved In-context Learning</span></br>
                                <span class="font-weight-bold">Man Luo</span>, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao</br>
                                Data Intelligence Journal 2024 </br>
				                <a href='https://arxiv.org/abs/2305.14128'>[Paper]</a> 
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col" style="margin-top: auto; margin-bottom: auto;">
                            <p class="lead">
                                <span class="font-weight-bold">Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering</span></br>
                                <span class="font-weight-bold">Man Luo*</span>, Yankai Zeng*, Pratyay Banerjee, Chitta Baral</br>
                                EMNLP 2021 </br>
				                <a href='https://arxiv.org/pdf/2109.04014.pdf'>[Paper]</a> <a href='https://github.com/luomancs/retriever_reader_for_okvqa'>[Code]</a>
                            </p>
                        </div>
                    </div>
                </div>
    <!-- </div> -->
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
</body>

</html>
